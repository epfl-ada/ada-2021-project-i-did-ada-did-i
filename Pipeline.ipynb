{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:ada] *",
      "language": "python",
      "name": "conda-env-ada-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9793a4a3-f51c-4777-ac02-703c1a1493ae",
        "5480330b-861d-4771-9109-ce1231250c7d"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "718c829d-a156-4c6f-8102-a47fbe43ddc4"
      },
      "source": [
        "# Proposed pipeline"
      ],
      "id": "718c829d-a156-4c6f-8102-a47fbe43ddc4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f58b18c-b3c4-41a7-a7fb-ba11a1b435cc"
      },
      "source": [
        "## A) Loading the data"
      ],
      "id": "6f58b18c-b3c4-41a7-a7fb-ba11a1b435cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XesiVsJzZDWF"
      },
      "source": [
        "### 1. Quotebank"
      ],
      "id": "XesiVsJzZDWF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H_-gPqAYI01",
        "outputId": "76ce41ac-7dfc-484c-ae19-221085b8b69b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import bz2\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import urllib.request as ul"
      ],
      "id": "0H_-gPqAYI01",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "cuONXghXYYEx",
        "outputId": "20fa7213-e3b5-4b32-db86-917114eddb73"
      },
      "source": [
        "# Load & format quotes from 2020\n",
        "#change on everyone's computer according to personal path\n",
        "path_to_file = '/content/drive/MyDrive/ADA/Quotebank/quotes-2020.json.bz2' \n",
        "\n",
        "list_of_quotes_dict = []\n",
        "count = 0\n",
        "sample_size = 1000  # Sample chosen for current experiments\n",
        "\n",
        "# Open the 2020 quotebank\n",
        "with bz2.open(path_to_file, 'rb') as s_file:\n",
        "    for instance in s_file:\n",
        "        if count == sample_size:\n",
        "            break\n",
        "        #print(instance)\n",
        "        decoded = json.loads(instance.decode('utf-8'))  # Decode each instance into a dictionary\n",
        "        #print(decoded[\"quoteID\"])\n",
        "        list_of_quotes_dict.append(decoded)\n",
        "        count += 1\n",
        "\n",
        "df_quotes = pd.DataFrame(list_of_quotes_dict)  # Turn list of entries into dataframe\n",
        "df_quotes.head()"
      ],
      "id": "cuONXghXYYEx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quoteID</th>\n",
              "      <th>quotation</th>\n",
              "      <th>speaker</th>\n",
              "      <th>qids</th>\n",
              "      <th>date</th>\n",
              "      <th>numOccurrences</th>\n",
              "      <th>probas</th>\n",
              "      <th>urls</th>\n",
              "      <th>phase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-28-000082</td>\n",
              "      <td>[ D ] espite the efforts of the partners to cr...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2020-01-28 08:04:05</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.7272], [Prime Minister Netanyahu, 0....</td>\n",
              "      <td>[http://israelnationalnews.com/News/News.aspx/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-16-000088</td>\n",
              "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
              "      <td>Sue Myrick</td>\n",
              "      <td>[Q367796]</td>\n",
              "      <td>2020-01-16 12:00:13</td>\n",
              "      <td>1</td>\n",
              "      <td>[[Sue Myrick, 0.8867], [None, 0.0992], [Ron Wy...</td>\n",
              "      <td>[http://thehill.com/opinion/international/4782...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-02-10-000142</td>\n",
              "      <td>... He (Madhav) also disclosed that the illega...</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2020-02-10 23:45:54</td>\n",
              "      <td>1</td>\n",
              "      <td>[[None, 0.8926], [Prakash Rai, 0.1074]]</td>\n",
              "      <td>[https://indianexpress.com/article/business/ec...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-02-15-000053</td>\n",
              "      <td>... [ I ] f it gets to the floor,</td>\n",
              "      <td>None</td>\n",
              "      <td>[]</td>\n",
              "      <td>2020-02-15 14:12:51</td>\n",
              "      <td>2</td>\n",
              "      <td>[[None, 0.581], [Andy Harris, 0.4191]]</td>\n",
              "      <td>[https://patriotpost.us/opinion/68622-trump-bu...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-24-000168</td>\n",
              "      <td>[ I met them ] when they just turned 4 and 7. ...</td>\n",
              "      <td>Meghan King Edmonds</td>\n",
              "      <td>[Q20684375]</td>\n",
              "      <td>2020-01-24 20:37:09</td>\n",
              "      <td>4</td>\n",
              "      <td>[[Meghan King Edmonds, 0.5446], [None, 0.2705]...</td>\n",
              "      <td>[https://people.com/parents/meghan-king-edmond...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             quoteID  ... phase\n",
              "0  2020-01-28-000082  ...     E\n",
              "1  2020-01-16-000088  ...     E\n",
              "2  2020-02-10-000142  ...     E\n",
              "3  2020-02-15-000053  ...     E\n",
              "4  2020-01-24-000168  ...     E\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFwxb1BwZGmK"
      },
      "source": [
        "### 2. Wikidata\n",
        "\n",
        "- job title (only this for milestone 2)\n",
        "- education level\n",
        "- area of interest\n",
        "- gender\n",
        "- etc"
      ],
      "id": "RFwxb1BwZGmK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymYSIcOjZKAE"
      },
      "source": [
        "# enjoying the loner task are you?"
      ],
      "id": "ymYSIcOjZKAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT0PuiX7WG-O"
      },
      "source": [
        "## B) Data exploration"
      ],
      "id": "mT0PuiX7WG-O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFvmoU-XoiN"
      },
      "source": [
        "# TODO"
      ],
      "id": "ucFvmoU-XoiN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHuixGYMWK2U"
      },
      "source": [
        "## C) Data preparation"
      ],
      "id": "EHuixGYMWK2U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf94WUmmWpiV"
      },
      "source": [
        "### 1. Clean data according to data exploration results"
      ],
      "id": "Lf94WUmmWpiV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b203389-451b-4285-b127-551b9fc2d02b"
      },
      "source": [
        "#### I) Load the sentences for each speakers with >X (tbd) quotes\n",
        "- Not for milestone 2, but afterwards will need info such as year, website, country and have to be able to keep them throughout the process"
      ],
      "id": "2b203389-451b-4285-b127-551b9fc2d02b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00c9d3f-c7d0-484c-b3f7-20f002a71140",
        "outputId": "6ffd79da-2c6e-4b31-9408-e39d9e033a2f"
      },
      "source": [
        "#code here\n",
        "\n",
        "min_quotes = 10  # Value of X\n",
        "\n",
        "counts = df_quotes.groupby(by=[\"speaker\"]).sum().reset_index()  # Count number of quotes per spreaker\n",
        "#print(counts.head(50))\n",
        "speakers_with_many_quotes = counts[counts['numOccurrences'] >= min_quotes]  # Select speakers with at least min_quotes\n",
        "speakers_with_many_quotes = speakers_with_many_quotes[speakers_with_many_quotes[\"speaker\"] != \"None\"]  # Remove \"None\" from speakers with many quotes\n",
        "#print(speakers_with_many_quotes.head(50))\n",
        "quotes_selected_speakers = df_quotes[df_quotes[\"speaker\"].isin(speakers_with_many_quotes[\"speaker\"])]  # Select quotes from speakers with many quotes\n",
        "quotes_selected_speakers.head(10)"
      ],
      "id": "c00c9d3f-c7d0-484c-b3f7-20f002a71140",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quoteID</th>\n",
              "      <th>quotation</th>\n",
              "      <th>speaker</th>\n",
              "      <th>qids</th>\n",
              "      <th>date</th>\n",
              "      <th>numOccurrences</th>\n",
              "      <th>probas</th>\n",
              "      <th>urls</th>\n",
              "      <th>phase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2020-03-18-000741</td>\n",
              "      <td>A face-to-face duty lawyer service provided by...</td>\n",
              "      <td>Mike Dwyer</td>\n",
              "      <td>[Q6379626]</td>\n",
              "      <td>2020-03-18 07:47:15</td>\n",
              "      <td>12</td>\n",
              "      <td>[[Mike Dwyer, 0.6042], [None, 0.3958]]</td>\n",
              "      <td>[http://www.balonnebeacon.com.au/news/tourism-...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2020-01-26-000499</td>\n",
              "      <td>a few of the candidates who will do better in ...</td>\n",
              "      <td>Dave Loebsack</td>\n",
              "      <td>[Q771586]</td>\n",
              "      <td>2020-01-26 13:21:36</td>\n",
              "      <td>11</td>\n",
              "      <td>[[Dave Loebsack, 0.9011], [None, 0.0949], [Joe...</td>\n",
              "      <td>[http://rss.cnn.com/~r/rss/cnn_allpolitics/~3/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>2020-01-10-005809</td>\n",
              "      <td>androids, surrogates or copies of real humans,</td>\n",
              "      <td>Pranav Mistry</td>\n",
              "      <td>[Q2722796]</td>\n",
              "      <td>2020-01-10 04:00:06</td>\n",
              "      <td>10</td>\n",
              "      <td>[[Pranav Mistry, 0.6069], [None, 0.3931]]</td>\n",
              "      <td>[http://news.cnet.com/how-to/samsungs-neon-exp...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>2020-01-28-006720</td>\n",
              "      <td>Apply for the privilege of having this entrepr...</td>\n",
              "      <td>Cindy Gallop</td>\n",
              "      <td>[Q5120529]</td>\n",
              "      <td>2020-01-28 13:00:00</td>\n",
              "      <td>22</td>\n",
              "      <td>[[Cindy Gallop, 0.921], [None, 0.079]]</td>\n",
              "      <td>[https://www.byronnews.com.au/news/womans-geni...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>2020-02-04-008518</td>\n",
              "      <td>At least 7 million lives could be saved over t...</td>\n",
              "      <td>Tedros Adhanom Ghebreyesus</td>\n",
              "      <td>[Q16196017]</td>\n",
              "      <td>2020-02-04 02:25:29</td>\n",
              "      <td>14</td>\n",
              "      <td>[[Tedros Adhanom Ghebreyesus, 0.9326], [None, ...</td>\n",
              "      <td>[https://www.miragenews.com/who-outlines-steps...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>2020-02-27-009528</td>\n",
              "      <td>But it really is that. The universe is a weird...</td>\n",
              "      <td>Melanie Johnston-Hollitt</td>\n",
              "      <td>[Q50505758, Q53953454]</td>\n",
              "      <td>2020-02-27 16:11:43</td>\n",
              "      <td>65</td>\n",
              "      <td>[[Melanie Johnston-Hollitt, 0.8937], [None, 0....</td>\n",
              "      <td>[http://andoveradvertiser.co.uk/news/national/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>2020-02-07-012379</td>\n",
              "      <td>but [ President ] Trump (was) eager to make a ...</td>\n",
              "      <td>President Donald Trump</td>\n",
              "      <td>[Q22686]</td>\n",
              "      <td>2020-02-07 23:05:05</td>\n",
              "      <td>1</td>\n",
              "      <td>[[President Donald Trump, 0.5698], [None, 0.43...</td>\n",
              "      <td>[http://uspolitics.einnews.com/article/5092030...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>2020-02-22-004519</td>\n",
              "      <td>But think of it: A man leaks classified inform...</td>\n",
              "      <td>President Donald Trump</td>\n",
              "      <td>[Q22686]</td>\n",
              "      <td>2020-02-22 16:58:48</td>\n",
              "      <td>4</td>\n",
              "      <td>[[President Donald Trump, 0.6539], [None, 0.21...</td>\n",
              "      <td>[http://rss.cnn.com/~r/rss/cnn_allpolitics/~3/...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>2020-04-14-008791</td>\n",
              "      <td>Can't wait for you to see what we've put toget...</td>\n",
              "      <td>Lady Gaga</td>\n",
              "      <td>[Q19848]</td>\n",
              "      <td>2020-04-14 21:58:48</td>\n",
              "      <td>2</td>\n",
              "      <td>[[Lady Gaga, 0.6251], [None, 0.3109], [Taylor ...</td>\n",
              "      <td>[https://www.nme.com/news/music/taylor-swift-c...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>2020-02-17-009693</td>\n",
              "      <td>Caroline and me were together from the very st...</td>\n",
              "      <td>Iain Stirling</td>\n",
              "      <td>[Q5980627]</td>\n",
              "      <td>2020-02-17 20:48:51</td>\n",
              "      <td>207</td>\n",
              "      <td>[[Iain Stirling, 0.7729], [None, 0.1476], [Car...</td>\n",
              "      <td>[https://www.eonline.com/news/1123804/love-isl...</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               quoteID  ... phase\n",
              "28   2020-03-18-000741  ...     E\n",
              "29   2020-01-26-000499  ...     E\n",
              "129  2020-01-10-005809  ...     E\n",
              "139  2020-01-28-006720  ...     E\n",
              "173  2020-02-04-008518  ...     E\n",
              "254  2020-02-27-009528  ...     E\n",
              "263  2020-02-07-012379  ...     E\n",
              "273  2020-02-22-004519  ...     E\n",
              "294  2020-04-14-008791  ...     E\n",
              "297  2020-02-17-009693  ...     E\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANIh7PNhUShx",
        "outputId": "261d0702-9ca7-49f5-d40f-95d7f773cb9a"
      },
      "source": [
        "quotes_selected_speakers.loc[139].quotation"
      ],
      "id": "ANIh7PNhUShx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Apply for the privilege of having this entrepreneurial, enterprising and creative woman work for you!'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0GTIluPOV8"
      },
      "source": [
        "### 2. Get each speaker's vocabulary"
      ],
      "id": "Dj0GTIluPOV8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb-Sg-laYEY1"
      },
      "source": [
        "#### I) Identify compound words and join them to tokenize them together\n",
        "\n",
        "E.g. \"United States\", \"data science\"\n",
        "\n",
        "Information sources:\n",
        "* [How to identify compound words](https://stackoverflow.com/questions/49403913/handling-compound-words-2-grams-using-nltk)\n",
        "* [Sense2vec](https://explosion.ai/blog/sense2vec-with-spacy)"
      ],
      "id": "lb-Sg-laYEY1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV7Zo_2zRHbS"
      },
      "source": [
        "# TODO Iris\n",
        "\n",
        "# 1) Identify compound words with sense2vec\n",
        "# 2) Format them to pass them to the MWE tokenizer"
      ],
      "id": "YV7Zo_2zRHbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b549892-64b6-4244-806d-57187fbcd962"
      },
      "source": [
        "#### II) Tokenise the sentences (turn them into single words)\n",
        "\n",
        "* [Some interesting information on tokenization](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9)\n",
        "* [Multi-Word Expression tokenizer](https://www.nltk.org/_modules/nltk/tokenize/mwe.html)"
      ],
      "id": "4b549892-64b6-4244-806d-57187fbcd962"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f88da2ff-3939-4cee-a7ad-4775de89537d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c9918210-835f-4459-8823-08051c28beb5"
      },
      "source": [
        "#your best code here\n",
        "# TODO Iris: use MWE tokenizer instead of word tokenizer to avoid splitting compound words!\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "quotes_selected_speakers[\"tokenized_quote\"] = quotes_selected_speakers.quotation.apply(word_tokenize)  # Tokenize quotes and add them in the dataframe as new column\n",
        "quotes_selected_speakers.head()"
      ],
      "id": "f88da2ff-3939-4cee-a7ad-4775de89537d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quoteID</th>\n",
              "      <th>quotation</th>\n",
              "      <th>speaker</th>\n",
              "      <th>qids</th>\n",
              "      <th>date</th>\n",
              "      <th>numOccurrences</th>\n",
              "      <th>probas</th>\n",
              "      <th>urls</th>\n",
              "      <th>phase</th>\n",
              "      <th>tokenized_quote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2020-03-18-000741</td>\n",
              "      <td>A face-to-face duty lawyer service provided by...</td>\n",
              "      <td>Mike Dwyer</td>\n",
              "      <td>[Q6379626]</td>\n",
              "      <td>2020-03-18 07:47:15</td>\n",
              "      <td>12</td>\n",
              "      <td>[[Mike Dwyer, 0.6042], [None, 0.3958]]</td>\n",
              "      <td>[http://www.balonnebeacon.com.au/news/tourism-...</td>\n",
              "      <td>E</td>\n",
              "      <td>[A, face-to-face, duty, lawyer, service, provi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2020-01-26-000499</td>\n",
              "      <td>a few of the candidates who will do better in ...</td>\n",
              "      <td>Dave Loebsack</td>\n",
              "      <td>[Q771586]</td>\n",
              "      <td>2020-01-26 13:21:36</td>\n",
              "      <td>11</td>\n",
              "      <td>[[Dave Loebsack, 0.9011], [None, 0.0949], [Joe...</td>\n",
              "      <td>[http://rss.cnn.com/~r/rss/cnn_allpolitics/~3/...</td>\n",
              "      <td>E</td>\n",
              "      <td>[a, few, of, the, candidates, who, will, do, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>2020-01-10-005809</td>\n",
              "      <td>androids, surrogates or copies of real humans,</td>\n",
              "      <td>Pranav Mistry</td>\n",
              "      <td>[Q2722796]</td>\n",
              "      <td>2020-01-10 04:00:06</td>\n",
              "      <td>10</td>\n",
              "      <td>[[Pranav Mistry, 0.6069], [None, 0.3931]]</td>\n",
              "      <td>[http://news.cnet.com/how-to/samsungs-neon-exp...</td>\n",
              "      <td>E</td>\n",
              "      <td>[androids, ,, surrogates, or, copies, of, real...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>2020-01-28-006720</td>\n",
              "      <td>Apply for the privilege of having this entrepr...</td>\n",
              "      <td>Cindy Gallop</td>\n",
              "      <td>[Q5120529]</td>\n",
              "      <td>2020-01-28 13:00:00</td>\n",
              "      <td>22</td>\n",
              "      <td>[[Cindy Gallop, 0.921], [None, 0.079]]</td>\n",
              "      <td>[https://www.byronnews.com.au/news/womans-geni...</td>\n",
              "      <td>E</td>\n",
              "      <td>[Apply, for, the, privilege, of, having, this,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>2020-02-04-008518</td>\n",
              "      <td>At least 7 million lives could be saved over t...</td>\n",
              "      <td>Tedros Adhanom Ghebreyesus</td>\n",
              "      <td>[Q16196017]</td>\n",
              "      <td>2020-02-04 02:25:29</td>\n",
              "      <td>14</td>\n",
              "      <td>[[Tedros Adhanom Ghebreyesus, 0.9326], [None, ...</td>\n",
              "      <td>[https://www.miragenews.com/who-outlines-steps...</td>\n",
              "      <td>E</td>\n",
              "      <td>[At, least, 7, million, lives, could, be, save...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               quoteID  ...                                    tokenized_quote\n",
              "28   2020-03-18-000741  ...  [A, face-to-face, duty, lawyer, service, provi...\n",
              "29   2020-01-26-000499  ...  [a, few, of, the, candidates, who, will, do, b...\n",
              "129  2020-01-10-005809  ...  [androids, ,, surrogates, or, copies, of, real...\n",
              "139  2020-01-28-006720  ...  [Apply, for, the, privilege, of, having, this,...\n",
              "173  2020-02-04-008518  ...  [At, least, 7, million, lives, could, be, save...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9793a4a3-f51c-4777-ac02-703c1a1493ae"
      },
      "source": [
        "#### III) Remove punctuation and stopwords, lowercase everything"
      ],
      "id": "9793a4a3-f51c-4777-ac02-703c1a1493ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f773f9eb-1e66-4283-b677-fbdaef0581aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "8bb07e4c-d2d2-43cb-a6c7-9a98547efc84"
      },
      "source": [
        "#should be easy enough, don't mess it up\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "EN_STOPWORDS = set(stopwords.words(\"english\"))  # English stopwords\n",
        "PUNCTUATION = set(string.punctuation)  # Punctuation\n",
        "\n",
        "def lowercase(l):\n",
        "    return [w.lower() for w in l]\n",
        "\n",
        "def remove_stopwords(l):\n",
        "    return [w for w in l if w.lower() not in EN_STOPWORDS]\n",
        "\n",
        "def remove_punctuation(l):\n",
        "    return [w for w in l if w not in PUNCTUATION]\n",
        "\n",
        "def process_tokens(l):\n",
        "    l = lowercase(l)  # Lowercase tokens\n",
        "    l = remove_stopwords(l)   # Remove stopwords from tokens\n",
        "    l = remove_punctuation(l)  # Remove punctuation from tokens\n",
        "    return l\n",
        "\n",
        "quotes_selected_speakers[\"tokenized_quote\"] = quotes_selected_speakers.tokenized_quote.apply(process_tokens)\n",
        "quotes_selected_speakers[[\"quotation\", \"tokenized_quote\"]].head()"
      ],
      "id": "f773f9eb-1e66-4283-b677-fbdaef0581aa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quotation</th>\n",
              "      <th>tokenized_quote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>A face-to-face duty lawyer service provided by...</td>\n",
              "      <td>[face-to-face, duty, lawyer, service, provided...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>a few of the candidates who will do better in ...</td>\n",
              "      <td>[candidates, better, part, world]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>androids, surrogates or copies of real humans,</td>\n",
              "      <td>[androids, surrogates, copies, real, humans]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>Apply for the privilege of having this entrepr...</td>\n",
              "      <td>[apply, privilege, entrepreneurial, enterprisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>At least 7 million lives could be saved over t...</td>\n",
              "      <td>[least, 7, million, lives, could, saved, next,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             quotation                                    tokenized_quote\n",
              "28   A face-to-face duty lawyer service provided by...  [face-to-face, duty, lawyer, service, provided...\n",
              "29   a few of the candidates who will do better in ...                  [candidates, better, part, world]\n",
              "129     androids, surrogates or copies of real humans,       [androids, surrogates, copies, real, humans]\n",
              "139  Apply for the privilege of having this entrepr...  [apply, privilege, entrepreneurial, enterprisi...\n",
              "173  At least 7 million lives could be saved over t...  [least, 7, million, lives, could, saved, next,..."
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5480330b-861d-4771-9109-ce1231250c7d"
      },
      "source": [
        "#### IV) Stem the words (ie 'eating', 'eats', 'ate' -> \"eat\")\n",
        "* [Source](https://towardsdatascience.com/stemming-corpus-with-nltk-7a6a6d02d3e5)"
      ],
      "id": "5480330b-861d-4771-9109-ce1231250c7d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bdd0370-09fa-494b-bda3-3ab87e7807bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a0b5cd2f-54f0-459f-d0bc-cb7c681a9d0c"
      },
      "source": [
        "#go champion, you can do it!\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# You may look into other stemming algorithms than this one, but note that no stemmer will give perfect results.\n",
        "SBS = SnowballStemmer(language='english')\n",
        "\n",
        "def stem_words(l):\n",
        "    return [SBS.stem(w) for w in l]\n",
        "\n",
        "quotes_selected_speakers[\"tokenized_quote\"] = quotes_selected_speakers.tokenized_quote.apply(stem_words)  # Stem tokens\n",
        "quotes_selected_speakers[[\"quotation\", \"tokenized_quote\"]].head()"
      ],
      "id": "3bdd0370-09fa-494b-bda3-3ab87e7807bb",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a1fe64115883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSBS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mquotes_selected_speakers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_quote\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquotes_selected_speakers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_quote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_words\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Stem tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mquotes_selected_speakers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quotation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tokenized_quote\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'quotes_selected_speakers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p60gp9kNRg3e"
      },
      "source": [
        "#### V) Pool tokens by speaker"
      ],
      "id": "p60gp9kNRg3e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXxcCFmQR6d0"
      },
      "source": [
        "# TODO"
      ],
      "id": "jXxcCFmQR6d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5bd42cd-f039-4f3c-8a8b-0c7a3f20116c"
      },
      "source": [
        "### 3. Assign an 'importance' score to each word"
      ],
      "id": "b5bd42cd-f039-4f3c-8a8b-0c7a3f20116c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVCSGx6YkDV"
      },
      "source": [
        "#### I) TF-IDF"
      ],
      "id": "WcVCSGx6YkDV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU5qL1evYnTN"
      },
      "source": [
        "# TODO"
      ],
      "id": "iU5qL1evYnTN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-joRKQnYqVd"
      },
      "source": [
        "#### II) Cambridge dictionary ECFR score and register"
      ],
      "id": "k-joRKQnYqVd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvSFU4ORe6pE",
        "outputId": "a7e44fd4-413e-4bf5-8611-f1ffac331ebd"
      },
      "source": [
        "# Add line for the word you want to look for \n",
        "levels, reg = retrieve_ECFR_levels_registers(\"abide\")\n",
        "print(levels, reg)"
      ],
      "id": "vvSFU4ORe6pE",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] ['old use']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdsbflcL0Ns2"
      },
      "source": [
        "def retrieve_ECFR_levels_registers(word):\n",
        "  \"\"\" Functions that retrives the ECFR level of the given word as well as its register\n",
        "      Input: \n",
        "        word - a string the word you want to inspect \n",
        "      Output: \n",
        "        levels - a numpy array containing strings, if empty no info was available\n",
        "        registers - a list of strings, if empty no info was available\n",
        "  \"\"\"\n",
        "  # Make a request to Cambridge dictionnary - simply add \"word\" to the url  \n",
        "  url = 'https://dictionary.cambridge.org/dictionary/english/'+word\n",
        "  # Websites don't like it when bots connect to them, so we have to pretend being a web browser! \n",
        "  req = ul.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  client = ul.urlopen(req)\n",
        "  # Read and store data into htmldata \n",
        "  htmldata = client.read()\n",
        "  client.close()\n",
        "\n",
        "  # Create a beautiful soup element and parse it as an html file\n",
        "  parsed_page = soup(htmldata, \"html.parser\")\n",
        "\n",
        "  # A bit of backgrond:\n",
        "  # In order to display the ECFR classification, the website uses <span> tags with a unique CSS class\n",
        "  # The class is called \"epp-xref dxref XX\", where XX is any of the levels A1 to C2\n",
        "  # Now we use 'bs4.find_all' to retrieve all span tag with the given class \n",
        "  # If none are returned, it means the level is not contained in the word \n",
        "\n",
        "  # Tag specific to the ECFR classification CSS class  \n",
        "  class_name_level = \"epp-xref dxref \" \n",
        "\n",
        "  # The ECFR classification spans from A1 to C2, test for each case\n",
        "  cases = np.array([\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"])\n",
        "  # We store the results in a bool vector from A1 to C2, corresponding to the level of the word  \n",
        "  results = []\n",
        "  for case in cases:\n",
        "    # This checks how many times the case is present on page\n",
        "    occurences = len(parsed_page.find_all(\"span\", class_= class_name_level+case))\n",
        "    # If the case is present more than 0 times, then the word can be classified in the given case \n",
        "    if occurences > 0:\n",
        "      results.append(True)\n",
        "    else: \n",
        "      results.append(False) \n",
        "\n",
        "  # Now let's retrieve the registers\n",
        "  # We can apply the same strategy, the register of the word is contained in CSS class \"usage dusage\"\n",
        "\n",
        "  class_name_register = \"usage dusage\"\n",
        "  parsed_registers = parsed_page.find_all(\"span\", class_=class_name_register)\n",
        "  registers = []\n",
        "  for register in parsed_registers:\n",
        "    # Simply extract the information between the tags \"span\" with class class_name_register\n",
        "    registers.append(register.string)\n",
        "\n",
        "  return cases[results], registers"
      ],
      "id": "YdsbflcL0Ns2",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nk5tnlUYyds"
      },
      "source": [
        "#### III) Log2"
      ],
      "id": "0nk5tnlUYyds"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCuS2JzlY5HJ"
      },
      "source": [
        "# TODO"
      ],
      "id": "wCuS2JzlY5HJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAsd1jJCZkNM"
      },
      "source": [
        "### 4. Create a vocabulary specific to each category in interest"
      ],
      "id": "WAsd1jJCZkNM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsCo8ib4Z0Pr"
      },
      "source": [
        "#### Classify speakers into categories of interest\n",
        "\n",
        "* Each speaker can belong to multiple categories"
      ],
      "id": "YsCo8ib4Z0Pr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfuNEu7YaHdH"
      },
      "source": [
        "# TODO after extracting wikidata in section A2"
      ],
      "id": "RfuNEu7YaHdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB0HSqdiZ0Hz"
      },
      "source": [
        "#### Create vocabulary\n",
        "\n",
        "Use:\n",
        "* Correlation of the use of certain words by the people in a given category\n",
        "* Words used mainly by people in given category and not so much by people from other categories\n",
        "\n",
        "Note:\n",
        "* Vocabularies may overlap\n",
        "\n",
        "Eventually:\n",
        "* Weight the vocabularies"
      ],
      "id": "yB0HSqdiZ0Hz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jurRq_Q3bQVU"
      },
      "source": [
        "#hopefully before friday 10pm "
      ],
      "id": "jurRq_Q3bQVU",
      "execution_count": null,
      "outputs": []
    }
  ]
}